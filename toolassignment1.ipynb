{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0374f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Requirement already satisfied: anthropic in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (0.52.2)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "\u001b[K     |████████████████████████████████| 187 kB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.0.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: httpx<1,>=0.25.0 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anthropic) (2.11.5)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anthropic) (0.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anthropic) (4.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anthropic) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->anthropic) (1.3.0)\n",
      "Requirement already satisfied: certifi in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 46.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp39-cp39-macosx_10_9_universal2.whl (201 kB)\n",
      "\u001b[K     |████████████████████████████████| 201 kB 60.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=e69367a83668dcdb973efc7126c4d69e84a0c12df03f88c3af99a280a5d38879\n",
      "  Stored in directory: /Users/anshika@browserstack.com/Library/Caches/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: urllib3, soupsieve, charset-normalizer, requests, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.13.4 charset-normalizer-3.4.2 requests-2.32.3 soupsieve-2.7 urllib3-2.4.0 wikipedia-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed9bc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshika@browserstack.com/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import os\n",
    "\n",
    "def generate_wikipedia_reading_list(research_topic, article_titles):\n",
    "    wikipedia_articles = []\n",
    "    for t in article_titles:\n",
    "        results = wikipedia.search(t)\n",
    "        try:\n",
    "            page = wikipedia.page(results[0])\n",
    "            title = page.title\n",
    "            url = page.url\n",
    "            wikipedia_articles.append({\"title\": title, \"url\": url})\n",
    "        except:\n",
    "            continue\n",
    "    add_to_research_reading_file(wikipedia_articles, research_topic)\n",
    "\n",
    "def add_to_research_reading_file(articles, topic):\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    with open(\"output/research_reading.md\", \"a\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"## {topic} \\n\")\n",
    "        for article in articles:\n",
    "            title = article[\"title\"]\n",
    "            url = article[\"url\"]\n",
    "            file.write(f\"* [{title}]({url}) \\n\")\n",
    "        file.write(f\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449c1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"generate_wikipedia_reading_list\",\n",
    "        \"description\": \"Takes a research topic and a list of potential article titles, and finds real Wikipedia articles related to the topic. Then saves them to a markdown file for later reading.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"research_topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The topic the user is researching\"\n",
    "                },\n",
    "                \"article_titles\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"description\": \"A list of possible Wikipedia article titles\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"research_topic\", \"article_titles\"]\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "def get_research_help(topic, num_articles=3):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful research assistant. When a user gives you a research topic, \"\n",
    "        \"suggest a list of realistic Wikipedia article titles related to the topic. \"\n",
    "        f\"Please return exactly {num_articles} titles in a JSON array, like: [\\\"Title 1\\\", \\\"Title 2\\\"]\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"I need help researching the topic '{topic}'. Please give me {num_articles} Wikipedia article titles.\"\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        system=system_prompt,\n",
    "        messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
    "    )\n",
    "\n",
    "    message = response.content[0].text.strip()\n",
    "\n",
    "    try:\n",
    "        article_titles = json.loads(message)\n",
    "        if isinstance(article_titles, list):\n",
    "            generate_wikipedia_reading_list(topic, article_titles)\n",
    "            print(f\"Added articles for topic: {topic}\")\n",
    "        else:\n",
    "            raise ValueError(\"Response was not a list\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse Claude's output as JSON. Here's the raw output:\")\n",
    "        print(message)\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d34d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added articles for topic: History of Hawaii\n",
      "Failed to parse Claude's output as JSON. Here's the raw output:\n",
      "Here are 5 relevant Wikipedia article titles for the topic 'Pirates Across The World':\n",
      "\n",
      "[\"History of Piracy\", \"Golden Age of Piracy\", \"List of Pirate Flags\", \"Piracy in the Caribbean\", \"Piracy in Southeast Asia\"]\n",
      "Error: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to parse Claude's output as JSON. Here's the raw output:\n",
      "Here are 4 Wikipedia article titles related to the topic \"Are animals conscious?\":\n",
      "\n",
      "[\"Consciousness in animals\", \"Animal cognition\", \"Sentience\", \"Philosophy of animal mind\"]\n",
      "Error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "get_research_help(\"History of Hawaii\", 3)\n",
    "get_research_help(\"Pirates Across The World\", 5)\n",
    "get_research_help(\"Are animals conscious?\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887242d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
